{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail  Real-Time Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster, ExecutionProfile, EXEC_PROFILE_DEFAULT\n",
    "from cassandra.policies import DCAwareRoundRobinPolicy, TokenAwarePolicy, DowngradingConsistencyRetryPolicy, ConsistencyLevel, RoundRobinPolicy\n",
    "from cassandra.query import tuple_factory\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType, DateType, LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set your Scylla IP(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPS=['172.19.0.2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the execution profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ExecutionProfile(\n",
    "    load_balancing_policy=TokenAwarePolicy(DCAwareRoundRobinPolicy(\"datacenter1\")),\n",
    "    retry_policy=DowngradingConsistencyRetryPolicy(), ##CHECK \n",
    "    consistency_level=ConsistencyLevel.LOCAL_QUORUM,\n",
    "    serial_consistency_level=ConsistencyLevel.LOCAL_SERIAL, ##CHECK LWT \n",
    "    request_timeout=20000,\n",
    "    row_factory=tuple_factory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(IPS,execution_profiles={EXEC_PROFILE_DEFAULT: profile})\n",
    "session = cluster.connect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace and connect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_keyspace = \"CREATE KEYSPACE IF NOT EXISTS tpcds WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1' : '1'};\"\n",
    "#session = cluster.connect()\n",
    "session.execute(create_keyspace)\n",
    "session = cluster.connect('tpcds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the tables from the CQL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: \"use tpcds\"\n",
      "Executing: \"drop table if exists call_center\"\n",
      "Executing: \"create table call_center(\n",
      "cc_call_center_sk int\n",
      ", cc_call_center_id text\n",
      ", cc_rec_start_date text\n",
      ", cc_rec_end_date text\n",
      ", cc_closed_date_sk int\n",
      ", cc_open_date_sk int\n",
      ", cc_name text\n",
      ", cc_class text\n",
      ", cc_employees text\n",
      ", cc_sq_ft int\n",
      ", cc_hours text\n",
      ", cc_manager text\n",
      ", cc_mkt_id int\n",
      ", cc_mkt_class text\n",
      ", cc_mkt_desc text\n",
      ", cc_market_manager text\n",
      ", cc_division int\n",
      ", cc_division_name text\n",
      ", cc_company int\n",
      ", cc_company_name text\n",
      ", cc_street_number text\n",
      ", cc_street_name text\n",
      ", cc_street_type text\n",
      ", cc_suite_number text\n",
      ", cc_city text\n",
      ", cc_county text\n",
      ", cc_state text\n",
      ", cc_zip text\n",
      ", cc_country text\n",
      ", cc_gmt_offset double\n",
      ", cc_tax_percentage double\n",
      ", junk text, PRIMARY KEY ((cc_call_center_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists catalog_page\"\n",
      "Executing: \"create table catalog_page(\n",
      "cp_catalog_page_sk int\n",
      ", cp_catalog_page_id text\n",
      ", cp_start_date_sk int\n",
      ", cp_end_date_sk int\n",
      ", cp_department text\n",
      ", cp_catalog_number int\n",
      ", cp_catalog_page_number int\n",
      ", cp_description text\n",
      ", cp_type text\n",
      ", junk text, PRIMARY KEY ((cp_catalog_page_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists catalog_returns\"\n",
      "Executing: \"create table catalog_returns\n",
      "(\n",
      "cr_returned_date_sk int,\n",
      "cr_returned_time_sk int,\n",
      "cr_item_sk int,\n",
      "cr_refunded_customer_sk int,\n",
      "cr_refunded_cdemo_sk int,\n",
      "cr_refunded_hdemo_sk int,\n",
      "cr_refunded_addr_sk int,\n",
      "cr_returning_customer_sk int,\n",
      "cr_returning_cdemo_sk int,\n",
      "cr_returning_hdemo_sk int,\n",
      "cr_returning_addr_sk int,\n",
      "cr_call_center_sk int,\n",
      "cr_catalog_page_sk int,\n",
      "cr_ship_mode_sk int,\n",
      "cr_warehouse_sk int,\n",
      "cr_reason_sk int,\n",
      "cr_order_number int,\n",
      "cr_return_quantity int,\n",
      "cr_return_amount double,\n",
      "cr_return_tax double,\n",
      "cr_return_amt_inc_tax double,\n",
      "cr_fee double,\n",
      "cr_return_ship_cost double,\n",
      "cr_refunded_cash double,\n",
      "cr_reversed_charge double,\n",
      "cr_store_credit double,\n",
      "cr_net_loss double\n",
      ", junk text, PRIMARY KEY ((cr_item_sk, cr_order_number))\n",
      ")\"\n",
      "Executing: \"drop table if exists catalog_sales\"\n",
      "Executing: \"create table catalog_sales\n",
      "(\n",
      "cs_sold_date_sk int,\n",
      "cs_sold_time_sk int,\n",
      "cs_ship_date_sk int,\n",
      "cs_bill_customer_sk int,\n",
      "cs_bill_cdemo_sk int,\n",
      "cs_bill_hdemo_sk int,\n",
      "cs_bill_addr_sk int,\n",
      "cs_ship_customer_sk int,\n",
      "cs_ship_cdemo_sk int,\n",
      "cs_ship_hdemo_sk int,\n",
      "cs_ship_addr_sk int,\n",
      "cs_call_center_sk int,\n",
      "cs_catalog_page_sk int,\n",
      "cs_ship_mode_sk int,\n",
      "cs_warehouse_sk int,\n",
      "cs_item_sk int,\n",
      "cs_promo_sk int,\n",
      "cs_order_number int,\n",
      "cs_quantity int,\n",
      "cs_wholesale_cost double,\n",
      "cs_list_price double,\n",
      "cs_sales_price double,\n",
      "cs_ext_discount_amt double,\n",
      "cs_ext_sales_price double,\n",
      "cs_ext_wholesale_cost double,\n",
      "cs_ext_list_price double,\n",
      "cs_ext_tax double,\n",
      "cs_coupon_amt double,\n",
      "cs_ext_ship_cost double,\n",
      "cs_net_paid double,\n",
      "cs_net_paid_inc_tax double,\n",
      "cs_net_paid_inc_ship double,\n",
      "cs_net_paid_inc_ship_tax double,\n",
      "cs_net_profit double\n",
      ", junk text, PRIMARY KEY ((cs_item_sk, cs_order_number))\n",
      ")\"\n",
      "Executing: \"drop table if exists customer_address\"\n",
      "Executing: \"create table customer_address\n",
      "(\n",
      "ca_address_sk int,\n",
      "ca_address_id text,\n",
      "ca_street_number text,\n",
      "ca_street_name text,\n",
      "ca_street_type text,\n",
      "ca_suite_number text,\n",
      "ca_city text,\n",
      "ca_county text,\n",
      "ca_state text,\n",
      "ca_zip text,\n",
      "ca_country text,\n",
      "ca_gmt_offset double,\n",
      "ca_location_type text\n",
      ", junk text, PRIMARY KEY ((ca_address_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists customer_demographics\"\n",
      "Executing: \"create table customer_demographics\n",
      "(\n",
      "cd_demo_sk int,\n",
      "cd_gender text,\n",
      "cd_marital_status text,\n",
      "cd_education_status text,\n",
      "cd_purchase_estimate int,\n",
      "cd_credit_rating text,\n",
      "cd_dep_count int,\n",
      "cd_dep_employed_count int,\n",
      "cd_dep_college_count int\n",
      ", junk text, PRIMARY KEY ((cd_demo_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists customer\"\n",
      "Executing: \"create table customer\n",
      "(\n",
      "c_customer_sk int,\n",
      "c_customer_id text,\n",
      "c_current_cdemo_sk int,\n",
      "c_current_hdemo_sk int,\n",
      "c_current_addr_sk int,\n",
      "c_first_shipto_date_sk int,\n",
      "c_first_sales_date_sk int,\n",
      "c_salutation text,\n",
      "c_first_name text,\n",
      "c_last_name text,\n",
      "c_preferred_cust_flag text,\n",
      "c_birth_day int,\n",
      "c_birth_month int,\n",
      "c_birth_year int,\n",
      "c_birth_country text,\n",
      "c_login text,\n",
      "c_email_address text,\n",
      "c_last_review_date text\n",
      ", junk text, PRIMARY KEY ((c_customer_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists date_dim\"\n",
      "Executing: \"create table date_dim\n",
      "(\n",
      "d_date_sk int,\n",
      "d_date_id text,\n",
      "d_date text,\n",
      "d_month_seq int,\n",
      "d_week_seq int,\n",
      "d_quarter_seq int,\n",
      "d_year int,\n",
      "d_dow int,\n",
      "d_moy int,\n",
      "d_dom int,\n",
      "d_qoy int,\n",
      "d_fy_year int,\n",
      "d_fy_quarter_seq int,\n",
      "d_fy_week_seq int,\n",
      "d_day_name text,\n",
      "d_quarter_name text,\n",
      "d_holiday text,\n",
      "d_weekend text,\n",
      "d_following_holiday text,\n",
      "d_first_dom int,\n",
      "d_last_dom int,\n",
      "d_same_day_ly int,\n",
      "d_same_day_lq int,\n",
      "d_current_day text,\n",
      "d_current_week text,\n",
      "d_current_month text,\n",
      "d_current_quarter text,\n",
      "d_current_year text\n",
      ", junk text, PRIMARY KEY ((d_date_sk))\n",
      ")\"\n",
      "Executing: \"create index on date_dim(d_date)\"\n",
      "Executing: \"drop table if exists household_demographics\"\n",
      "Executing: \"create table household_demographics\n",
      "(\n",
      "hd_demo_sk int,\n",
      "hd_income_band_sk int,\n",
      "hd_buy_potential text,\n",
      "hd_dep_count int,\n",
      "hd_vehicle_count int\n",
      ", junk text, PRIMARY KEY ((hd_demo_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists income_band\"\n",
      "Executing: \"create table income_band(\n",
      "ib_income_band_sk int\n",
      ", ib_lower_bound int\n",
      ", ib_upper_bound int\n",
      ", junk text, PRIMARY KEY ((ib_income_band_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists inventory\"\n",
      "Executing: \"create table inventory\n",
      "(\n",
      "inv_date_sk int,\n",
      "inv_item_sk int,\n",
      "inv_warehouse_sk int,\n",
      "inv_quantity_on_hand int\n",
      ", junk text, PRIMARY KEY ((inv_date_sk, inv_item_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists item\"\n",
      "Executing: \"create table item\n",
      "(\n",
      "i_item_sk int,\n",
      "i_item_id text,\n",
      "i_rec_start_date text,\n",
      "i_rec_end_date text,\n",
      "i_item_desc text,\n",
      "i_current_price double,\n",
      "i_wholesale_cost double,\n",
      "i_brand_id int,\n",
      "i_brand text,\n",
      "i_class_id int,\n",
      "i_class text,\n",
      "i_category_id int,\n",
      "i_category text,\n",
      "i_manufact_id int,\n",
      "i_manufact text,\n",
      "i_size text,\n",
      "i_formulation text,\n",
      "i_color text,\n",
      "i_units text,\n",
      "i_container text,\n",
      "i_manager_id int,\n",
      "i_product_name text\n",
      ", junk text, PRIMARY KEY ((i_item_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists promotion\"\n",
      "Executing: \"create table promotion\n",
      "(\n",
      "p_promo_sk int,\n",
      "p_promo_id text,\n",
      "p_start_date_sk int,\n",
      "p_end_date_sk int,\n",
      "p_item_sk int,\n",
      "p_cost double,\n",
      "p_response_target int,\n",
      "p_promo_name text,\n",
      "p_channel_dmail text,\n",
      "p_channel_email text,\n",
      "p_channel_catalog text,\n",
      "p_channel_tv text,\n",
      "p_channel_radio text,\n",
      "p_channel_press text,\n",
      "p_channel_event text,\n",
      "p_channel_demo text,\n",
      "p_channel_details text,\n",
      "p_purpose text,\n",
      "p_discount_active text\n",
      ", junk text, PRIMARY KEY ((p_promo_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists reason\"\n",
      "Executing: \"create table reason(\n",
      "r_reason_sk int\n",
      ", r_reason_id text\n",
      ", r_reason_desc text\n",
      ", junk text, PRIMARY KEY ((r_reason_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists ship_mode\"\n",
      "Executing: \"create table ship_mode(\n",
      "sm_ship_mode_sk int\n",
      ", sm_ship_mode_id text\n",
      ", sm_type text\n",
      ", sm_code text\n",
      ", sm_carrier text\n",
      ", sm_contract text\n",
      ", junk text, PRIMARY KEY ((sm_ship_mode_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists store_returns\"\n",
      "Executing: \"create table store_returns\n",
      "(\n",
      "sr_returned_date_sk int,\n",
      "sr_return_time_sk int,\n",
      "sr_item_sk int,\n",
      "sr_customer_sk int,\n",
      "sr_cdemo_sk int,\n",
      "sr_hdemo_sk int,\n",
      "sr_addr_sk int,\n",
      "sr_store_sk int,\n",
      "sr_reason_sk int,\n",
      "sr_ticket_number int,\n",
      "sr_return_quantity int,\n",
      "sr_return_amt double,\n",
      "sr_return_tax double,\n",
      "sr_return_amt_inc_tax double,\n",
      "sr_fee double,\n",
      "sr_return_ship_cost double,\n",
      "sr_refunded_cash double,\n",
      "sr_reversed_charge double,\n",
      "sr_store_credit double,\n",
      "sr_net_loss double\n",
      ", junk text, PRIMARY KEY ((sr_item_sk, sr_ticket_number))\n",
      ")\"\n",
      "Executing: \"drop table if exists store_sales\"\n",
      "Executing: \"create table store_sales\n",
      "(\n",
      "ss_sold_date_sk int,\n",
      "ss_sold_time_sk int,\n",
      "ss_item_sk int,\n",
      "ss_customer_sk int,\n",
      "ss_cdemo_sk int,\n",
      "ss_hdemo_sk int,\n",
      "ss_addr_sk int,\n",
      "ss_store_sk int,\n",
      "ss_promo_sk int,\n",
      "ss_ticket_number int,\n",
      "ss_quantity int,\n",
      "ss_wholesale_cost double,\n",
      "ss_list_price double,\n",
      "ss_sales_price double,\n",
      "ss_ext_discount_amt double,\n",
      "ss_ext_sales_price double,\n",
      "ss_ext_wholesale_cost double,\n",
      "ss_ext_list_price double,\n",
      "ss_ext_tax double,\n",
      "ss_coupon_amt double,\n",
      "ss_net_paid double,\n",
      "ss_net_paid_inc_tax double,\n",
      "ss_net_profit double\n",
      ", junk text, PRIMARY KEY ((ss_item_sk, ss_ticket_number))\n",
      ")\"\n",
      "Executing: \"drop table if exists store\"\n",
      "Executing: \"create table store\n",
      "(\n",
      "s_store_sk int,\n",
      "s_store_id text,\n",
      "s_rec_start_date text,\n",
      "s_rec_end_date text,\n",
      "s_closed_date_sk int,\n",
      "s_store_name text,\n",
      "s_number_employees int,\n",
      "s_floor_space int,\n",
      "s_hours text,\n",
      "s_manager text,\n",
      "s_market_id int,\n",
      "s_geography_class text,\n",
      "s_market_desc text,\n",
      "s_market_manager text,\n",
      "s_division_id int,\n",
      "s_division_name text,\n",
      "s_company_id int,\n",
      "s_company_name text,\n",
      "s_street_number text,\n",
      "s_street_name text,\n",
      "s_street_type text,\n",
      "s_suite_number text,\n",
      "s_city text,\n",
      "s_county text,\n",
      "s_state text,\n",
      "s_zip text,\n",
      "s_country text,\n",
      "s_gmt_offset double,\n",
      "s_tax_precentage double\n",
      ", junk text, PRIMARY KEY ((s_store_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists time_dim\"\n",
      "Executing: \"create table time_dim\n",
      "(\n",
      "t_time_sk int,\n",
      "t_time_id text,\n",
      "t_time int,\n",
      "t_hour int,\n",
      "t_minute int,\n",
      "t_second int,\n",
      "t_am_pm text,\n",
      "t_shift text,\n",
      "t_sub_shift text,\n",
      "t_meal_time text\n",
      ", junk text, PRIMARY KEY ((t_time_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists warehouse\"\n",
      "Executing: \"create table warehouse(\n",
      "w_warehouse_sk int\n",
      ", w_warehouse_id text\n",
      ", w_warehouse_name text\n",
      ", w_warehouse_sq_ft int\n",
      ", w_street_number text\n",
      ", w_street_name text\n",
      ", w_street_type text\n",
      ", w_suite_number text\n",
      ", w_city text\n",
      ", w_county text\n",
      ", w_state text\n",
      ", w_zip text\n",
      ", w_country text\n",
      ", w_gmt_offset double\n",
      ", junk text, PRIMARY KEY ((w_warehouse_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists web_page\"\n",
      "Executing: \"create table web_page(\n",
      "wp_web_page_sk int\n",
      ", wp_web_page_id text\n",
      ", wp_rec_start_date text\n",
      ", wp_rec_end_date text\n",
      ", wp_creation_date_sk int\n",
      ", wp_access_date_sk int\n",
      ", wp_autogen_flag text\n",
      ", wp_customer_sk int\n",
      ", wp_url text\n",
      ", wp_type text\n",
      ", wp_char_count int\n",
      ", wp_link_count int\n",
      ", wp_image_count int\n",
      ", wp_max_ad_count int\n",
      ", junk text, PRIMARY KEY ((wp_web_page_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists web_returns\"\n",
      "Executing: \"create table web_returns\n",
      "(\n",
      "wr_returned_date_sk int,\n",
      "wr_returned_time_sk int,\n",
      "wr_item_sk int,\n",
      "wr_refunded_customer_sk int,\n",
      "wr_refunded_cdemo_sk int,\n",
      "wr_refunded_hdemo_sk int,\n",
      "wr_refunded_addr_sk int,\n",
      "wr_returning_customer_sk int,\n",
      "wr_returning_cdemo_sk int,\n",
      "wr_returning_hdemo_sk int,\n",
      "wr_returning_addr_sk int,\n",
      "wr_web_page_sk int,\n",
      "wr_reason_sk int,\n",
      "wr_order_number int,\n",
      "wr_return_quantity int,\n",
      "wr_return_amt double,\n",
      "wr_return_tax double,\n",
      "wr_return_amt_inc_tax double,\n",
      "wr_fee double,\n",
      "wr_return_ship_cost double,\n",
      "wr_refunded_cash double,\n",
      "wr_reversed_charge double,\n",
      "wr_account_credit double,\n",
      "wr_net_loss double\n",
      ", junk text, PRIMARY KEY ((wr_order_number, wr_item_sk))\n",
      ")\"\n",
      "Executing: \"drop table if exists web_sales\"\n",
      "Executing: \"create table web_sales\n",
      "(\n",
      "ws_sold_date_sk int,\n",
      "ws_sold_time_sk int,\n",
      "ws_ship_date_sk int,\n",
      "ws_item_sk int,\n",
      "ws_bill_customer_sk int,\n",
      "ws_bill_cdemo_sk int,\n",
      "ws_bill_hdemo_sk int,\n",
      "ws_bill_addr_sk int,\n",
      "ws_ship_customer_sk int,\n",
      "ws_ship_cdemo_sk int,\n",
      "ws_ship_hdemo_sk int,\n",
      "ws_ship_addr_sk int,\n",
      "ws_web_page_sk int,\n",
      "ws_web_site_sk int,\n",
      "ws_ship_mode_sk int,\n",
      "ws_warehouse_sk int,\n",
      "ws_promo_sk int,\n",
      "ws_order_number int,\n",
      "ws_quantity int,\n",
      "ws_wholesale_cost double,\n",
      "ws_list_price double,\n",
      "ws_sales_price double,\n",
      "ws_ext_discount_amt double,\n",
      "ws_ext_sales_price double,\n",
      "ws_ext_wholesale_cost double,\n",
      "ws_ext_list_price double,\n",
      "ws_ext_tax double,\n",
      "ws_coupon_amt double,\n",
      "ws_ext_ship_cost double,\n",
      "ws_net_paid double,\n",
      "ws_net_paid_inc_tax double,\n",
      "ws_net_paid_inc_ship double,\n",
      "ws_net_paid_inc_ship_tax double,\n",
      "ws_net_profit double\n",
      ", junk text, PRIMARY KEY ((ws_item_sk, ws_order_number))\n",
      ")\"\n",
      "Executing: \"drop table if exists web_site\"\n",
      "Executing: \"create table web_site\n",
      "(\n",
      "web_site_sk int,\n",
      "web_site_id text,\n",
      "web_rec_start_date text,\n",
      "web_rec_end_date text,\n",
      "web_name text,\n",
      "web_open_date_sk int,\n",
      "web_close_date_sk int,\n",
      "web_class text,\n",
      "web_manager text,\n",
      "web_mkt_id int,\n",
      "web_mkt_class text,\n",
      "web_mkt_desc text,\n",
      "web_market_manager text,\n",
      "web_company_id int,\n",
      "web_company_name text,\n",
      "web_street_number text,\n",
      "web_street_name text,\n",
      "web_street_type text,\n",
      "web_suite_number text,\n",
      "web_city text,\n",
      "web_county text,\n",
      "web_state text,\n",
      "web_zip text,\n",
      "web_country text,\n",
      "web_gmt_offset double,\n",
      "web_tax_percentage double\n",
      ", junk text, PRIMARY KEY ((web_site_sk))\n",
      ")\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('ddl.cql', mode='r') as f:\n",
    "    txt = f.read()\n",
    "    stmts = txt.split(r';')\n",
    "    for i in stmts:\n",
    "        stmt = i.strip()\n",
    "        if stmt != '':\n",
    "            print('Executing: \"' + stmt + '\"')\n",
    "            session.execute(stmt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, run the Generate_Dimension_Tables_CSV notebook first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading using Spark\n",
    "#### Spark Integration and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/datastax/spark-cassandra-connector/blob/master/doc/reference.md#write-tuning-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Tuning Parameters\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Property Name\t                                             Default\t        Description\n",
    "spark.cassandra.output.batch.grouping.buffer.size\t         1000\t            How many batches per single Spark task can be stored in memory before sending to Cassandra\n",
    "spark.cassandra.output.batch.grouping.key\t                 Partition\t        Determines how insert statements are grouped into batches. Available values are:\n",
    "                                                                                none : a batch may contain any statements\n",
    "                                                                                replica_set : a batch may contain only statements to be written to the same replica set\n",
    "                                                                                partition : a batch may contain only statements for rows sharing the same partition key value\n",
    "                                                                                \n",
    "spark.cassandra.output.batch.size.bytes\t                      1024\t            Maximum total size of the batch in bytes. Overridden by spark.cassandra.output.batch.size.rows\n",
    "spark.cassandra.output.batch.size.rows\t                      None\t            Number of rows per single batch. The default is 'auto' which means the connector will adjust the number of rows based on the amount of data in each row\n",
    "spark.cassandra.output.concurrent.writes\t                  5\t                Maximum number of batches executed in parallel by a single Spark task\n",
    "spark.cassandra.output.consistency.level\t                  LOCAL_QUORUM\t    Consistency level for writing\n",
    "spark.cassandra.output.ifNotExists\t                          false\t            Determines that the INSERT operation is not performed if a row with the same primary key already exists. Using the feature incurs a performance hit.\n",
    "spark.cassandra.output.ignoreNulls\t                          false\t            In Cassandra >= 2.2 null values can be left as unset in bound statements. Setting this to True will cause all null values to be left as unset rather than bound. \n",
    "                                                                                For finer control see the CassandraOption class\n",
    "                                                                                \n",
    "spark.cassandra.output.metrics\t                              True\t            Sets whether to record connector specific metrics on write\n",
    "spark.cassandra.output.throughputMBPerSec\t                  None\t            *(Floating points allowed)* Maximum write throughput allowed per single core in MB/s.\n",
    "                                                                                Limit this on long (+8 hour) runs to 70% of your max throughput as seen on a smaller job for stability\n",
    "                                                                                \n",
    "spark.cassandra.output.timestamp                              0\t                Timestamp (microseconds since epoch) of the write. If not specified, the time that the write occurred is used. A value of 0 means time of write.\n",
    "spark.cassandra.output.ttl\t                                  0\t                Time To Live(TTL) assigned to writes to Cassandra. A value of 0 means no TTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/11/29 09:06:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"IoT-Scylla\")\\\n",
    "    .config(\"spark.jars\", \"target/scala-2.12/spark3-scylla4-example-assembly-0.1.jar\")\\\n",
    "    .config(\"spark.cassandra.connection.host\", ','.join(IPS))\\\n",
    "    .config('spark.cassandra.output.batch.grouping.buffer.size','1000')\\\n",
    "    .config('spark.cassandra.output.batch.grouping.key','Partition')\\\n",
    "    .config('spark.cassandra.output.batch.size.bytes','1024')\\\n",
    "    .config('spark.cassandra.output.batch.size.rows','auto')\\\n",
    "    .config('spark.cassandra.output.concurrent.writes','10')\\\n",
    "    .config('spark.cassandra.output.consistency.level','LOCAL_QUORUM')\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining schemas and loading the parquet files into Scylla using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catalog_page\n",
      "store\n",
      "customer_address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_center\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'schema_web_site' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'schema_web_site' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"catalog_page\")\n",
    "schema_catalog_page= StructType([StructField(\"cp_catalog_page_sk\",IntegerType(),True),StructField(\"cp_catalog_page_id\",StringType(),True),StructField(\"cp_start_date_sk\",IntegerType(),True),StructField(\"cp_end_date_sk\",IntegerType(),True),StructField(\"cp_department\",StringType(),True),StructField(\"cp_catalog_number\",IntegerType(),True),StructField(\"cp_catalog_page_number\",IntegerType(),True),StructField(\"cp_description\",StringType(),True),StructField(\"cp_type\",StringType(),True)])\n",
    "csv_catalog_page = spark.read.format(\"parquet\").schema(schema_catalog_page).load(\"./data/data_parquet/catalog_page/\", delimiter=\"|\")            \n",
    "csv_catalog_page.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"catalog_page\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "\n",
    "\n",
    "print(\"store\")\n",
    "\n",
    "schema_store= StructType([StructField(\"s_store_sk\",IntegerType(),True),StructField(\"s_store_id\",StringType(),True),StructField(\"s_rec_start_date\",StringType(),True),StructField(\"s_rec_end_date\",StringType(),True),StructField(\"s_closed_date_sk\",IntegerType(),True),StructField(\"s_store_name\",StringType(),True),StructField(\"s_number_employees\",IntegerType(),True),StructField(\"s_floor_space\",IntegerType(),True),StructField(\"s_hours\",StringType(),True),StructField(\"s_manager\",StringType(),True),StructField(\"s_market_id\",IntegerType(),True),StructField(\"s_geography_class\",StringType(),True),StructField(\"s_market_desc\",StringType(),True),StructField(\"s_market_manager\",StringType(),True),StructField(\"s_division_id\",IntegerType(),True),StructField(\"s_division_name\",StringType(),True),StructField(\"s_company_id\",IntegerType(),True),StructField(\"s_company_name\",StringType(),True),StructField(\"s_street_number\",StringType(),True),StructField(\"s_street_name\",StringType(),True),StructField(\"s_street_type\",StringType(),True),StructField(\"s_suite_number\",StringType(),True),StructField(\"s_city\",StringType(),True),StructField(\"s_county\",StringType(),True),StructField(\"s_state\",StringType(),True),StructField(\"s_zip\",StringType(),True),StructField(\"s_country\",StringType(),True),StructField(\"s_gmt_offset\",FloatType(),True),StructField(\"s_tax_precentage\",FloatType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_store = spark.read.format(\"parquet\").schema(schema_store).load(\"./data/data_parquet/store/\", delimiter=\"|\")            \n",
    "csv_store.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"store\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "\n",
    "\n",
    "print(\"customer_address\")\n",
    "schema_customer_address = StructType([ StructField(\"ca_address_sk\",IntegerType(),True), StructField(\"ca_address_id\",StringType(),True), StructField(\"ca_street_number\",StringType(),True), StructField(\"ca_street_name\",StringType(),True), StructField(\"ca_street_type\",StringType(),True), StructField(\"ca_suite_number\",StringType(),True), StructField(\"ca_city\",StringType(),True), StructField(\"ca_county\",StringType(),True), StructField(\"ca_state\",StringType(),True), StructField(\"ca_zip\",StringType(),True), StructField(\"ca_country\",StringType(),True), StructField(\"ca_gmt_offset\",FloatType(),True), StructField(\"ca_location_type\",StringType(),True), StructField(\"junk\",StringType(),True), ])\n",
    "csv_customer_address = spark.read.format(\"parquet\").schema(schema_customer_address).load(\"./data/data_parquet/customer_address/\", delimiter=\"|\")\n",
    "csv_customer_address.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_address\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "\n",
    "print(\"customer\")\n",
    "schema_customer = StructType([ StructField(\"c_customer_sk\",IntegerType(),True), StructField(\"c_customer_id\",StringType(),True), StructField(\"c_current_cdemo_sk\",IntegerType(),True), StructField(\"c_current_hdemo_sk\",IntegerType(),True), StructField(\"c_current_addr_sk\",IntegerType(),True), StructField(\"c_first_shipto_date_sk\",IntegerType(),True), StructField(\"c_first_sales_date_sk\",IntegerType(),True), StructField(\"c_salutation\",StringType(),True), StructField(\"c_first_name\",StringType(),True), StructField(\"c_last_name\",StringType(),True), StructField(\"c_preferred_cust_flag\",StringType(),True), StructField(\"c_birth_day\",IntegerType(),True), StructField(\"c_birth_month\",IntegerType(),True), StructField(\"c_birth_year\",IntegerType(),True), StructField(\"c_birth_country\",StringType(),True), StructField(\"c_login\",StringType(),True), StructField(\"c_email_address\",StringType(),True), StructField(\"c_last_review_date\",StringType(),True), StructField(\"junk\",StringType(),True) ])\n",
    "csv_customer = spark.read.format(\"parquet\").schema(schema_customer).load(\"./data/data_parquet/customer/\", delimiter=\"|\")\n",
    "csv_customer.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"customer_demo\")\n",
    "schema_customer_demo = StructType([ StructField(\"cd_demo_sk\",IntegerType(),True), StructField(\"cd_gender\",StringType(),True), StructField(\"cd_marital_status\",StringType(),True), StructField(\"cd_education_status\",StringType(),True), StructField(\"cd_purchase_estimate\",IntegerType(),True), StructField(\"cd_credit_rating\",StringType(),True), StructField(\"cd_dep_count\",IntegerType(),True), StructField(\"cd_dep_employed_count\",IntegerType(),True), StructField(\"cd_dep_college_count\",IntegerType(),True), StructField(\"junk\",StringType(),True), ])\n",
    "csv_customer_demo = spark.read.format(\"parquet\").schema(schema_customer_demo).load(\"./data/data_parquet/customer_demographics/\", delimiter=\"|\")\n",
    "csv_customer_demo.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_demographics\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "# print(\"inventory\")\n",
    "# schema_inventory = StructType([ StructField(\"inv_date_sk\",IntegerType(),True), StructField(\"inv_item_sk\",IntegerType(),True), StructField(\"inv_warehouse_sk\",IntegerType(),True), StructField(\"inv_quantity_on_hand\",IntegerType(),True), StructField(\"junk\",IntegerType(),True),  ])\n",
    "# csv_inventory = spark.read.format(\"parquet\").schema(schema_inventory).load(\"./data/data_parquet/inventory/\", delimiter=\"|\")\n",
    "# csv_inventory.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"inventory\",keyspace=\"tpcds\").mode(\"append\").save()   \n",
    "print(\"call_center\")\n",
    "schema_call_center = StructType([StructField(\"cc_call_center_sk\",IntegerType(),True),StructField(\"cc_call_center_id\",StringType(),True),StructField(\"cc_rec_start_date\",StringType(),True),StructField(\"cc_rec_end_date\",StringType(),True),StructField(\"cc_closed_date_sk\",IntegerType(),True),StructField(\"cc_open_date_sk\",IntegerType(),True),StructField(\"cc_name\",StringType(),True),StructField(\"cc_class\",StringType(),True),StructField(\"cc_employees\",StringType(),True),StructField(\"cc_sq_ft\",IntegerType(),True),StructField(\"cc_hours\",StringType(),True),StructField(\"cc_manager\",StringType(),True),StructField(\"cc_mkt_id\",IntegerType(),True),StructField(\"cc_mkt_class\",StringType(),True),StructField(\"cc_mkt_desc\",StringType(),True),StructField(\"cc_market_manager\",StringType(),True),StructField(\"cc_division\",IntegerType(),True),StructField(\"cc_division_name\",StringType(),True),StructField(\"cc_company\",IntegerType(),True),StructField(\"cc_company_name\",StringType(),True),StructField(\"cc_street_number\",StringType(),True),StructField(\"cc_street_name\",StringType(),True),StructField(\"cc_street_type\",StringType(),True),StructField(\"cc_suite_number\",StringType(),True),StructField(\"cc_city\",StringType(),True),StructField(\"cc_county\",StringType(),True),StructField(\"cc_state\",StringType(),True),StructField(\"cc_zip\",StringType(),True),StructField(\"cc_country\",StringType(),True),StructField(\"cc_gmt_offset\",FloatType(),True),StructField(\"cc_tax_percentage\",FloatType(),True),StructField(\"junk\",StringType(),True),])\n",
    "csv_call_center = spark.read.format(\"parquet\").schema(schema_call_center).load(\"./data/data_parquet/call_center/\", delimiter=\"|\")            \n",
    "csv_call_center.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"call_center\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"schema_web_site\")\n",
    "schema_web_site = StructType([StructField(\"web_site_sk\",IntegerType(),True),StructField(\"web_site_id\",StringType(),True),StructField(\"web_rec_start_date\",StringType(),True),StructField(\"web_rec_end_date\",StringType(),True),StructField(\"web_name\",StringType(),True),StructField(\"web_open_date_sk\",IntegerType(),True),StructField(\"web_close_date_sk\",IntegerType(),True),StructField(\"web_class\",StringType(),True),StructField(\"web_manager\",StringType(),True),StructField(\"web_mkt_id\",IntegerType(),True),StructField(\"web_mkt_class\",StringType(),True),StructField(\"web_mkt_desc\",StringType(),True),StructField(\"web_market_manager\",StringType(),True),StructField(\"web_company_id\",IntegerType(),True),StructField(\"web_company_name\",StringType(),True),StructField(\"web_street_number\",StringType(),True),StructField(\"web_street_name\",StringType(),True),StructField(\"web_street_type\",StringType(),True),StructField(\"web_suite_number\",StringType(),True),StructField(\"web_city\",StringType(),True),StructField(\"web_county\",StringType(),True),StructField(\"web_state\",StringType(),True),StructField(\"web_zip\",StringType(),True),StructField(\"web_country\",StringType(),True),StructField(\"web_gmt_offset\",FloatType(),True),StructField(\"web_tax_percentage\",FloatType(),True),StructField(\"junk\",StringType(),True),   ])  \n",
    "csv_web_site = spark.read.format(\"parquet\").schema(schema_web_site).load(\"./data/data_parquet/web_site/\", delimiter=\"|\")            \n",
    "csv_web_site.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_site\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"schema_web_page\")\n",
    "\n",
    "schema_web_page = StructType([StructField(\"wp_web_page_sk\",IntegerType(),True),StructField(\"wp_web_page_id\",StringType(),True),StructField(\"wp_rec_start_date\",StringType(),True),StructField(\"wp_rec_end_date\",StringType(),True),StructField(\"wp_creation_date_sk\",IntegerType(),True),StructField(\"wp_access_date_sk\",IntegerType(),True),StructField(\"wp_autogen_flag\",StringType(),True),StructField(\"wp_customer_sk\",IntegerType(),True),StructField(\"wp_url\",StringType(),True),StructField(\"wp_type\",StringType(),True),StructField(\"wp_char_count\",IntegerType(),True),StructField(\"wp_link_count\",IntegerType(),True),StructField(\"wp_image_count\",IntegerType(),True),StructField(\"wp_max_ad_count\",IntegerType(),True),StructField(\"junk\",StringType(),True),])  \n",
    "csv_web_page = spark.read.format(\"parquet\").schema(schema_web_page).load(\"./data/data_parquet/web_page/\", delimiter=\"|\")            \n",
    "csv_web_page.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_page\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"schema_warehouse\")\n",
    "\n",
    "schema_warehouse = StructType([StructField(\"w_warehouse_sk\",IntegerType(),True),StructField(\"w_warehouse_id\",StringType(),True),StructField(\"w_warehouse_name\",StringType(),True),StructField(\"w_warehouse_sq_ft\",IntegerType(),True),StructField(\"w_street_number\",StringType(),True),StructField(\"w_street_name\",StringType(),True),StructField(\"w_street_type\",StringType(),True),StructField(\"w_suite_number\",StringType(),True),StructField(\"w_city\",StringType(),True),StructField(\"w_county\",StringType(),True),StructField(\"w_state\",StringType(),True),StructField(\"w_zip\",StringType(),True),StructField(\"w_country\",StringType(),True),StructField(\"w_gmt_offset\",FloatType(),True),StructField(\"junk\",StringType(),True), ])            \n",
    "csv_warehouse = spark.read.format(\"parquet\").schema(schema_warehouse).load(\"./data/data_parquet/warehouse/\", delimiter=\"|\")            \n",
    "csv_warehouse.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"warehouse\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"schema_time\")\n",
    "\n",
    "schema_time= StructType([StructField(\"t_time_sk\",IntegerType(),True),StructField(\"t_time_id\",StringType(),True),StructField(\"t_time\",IntegerType(),True),StructField(\"t_hour\",IntegerType(),True),StructField(\"t_minute\",IntegerType(),True),StructField(\"t_second\",IntegerType(),True),StructField(\"t_am_pm\",StringType(),True),StructField(\"t_shift\",StringType(),True),StructField(\"t_sub_shift\",StringType(),True),StructField(\"t_meal_time\",StringType(),True),StructField(\"junk\",StringType(),True), ])            \n",
    "csv_time_dim = spark.read.format(\"parquet\").schema(schema_time).load(\"./data/data_parquet/time_dim/\", delimiter=\"|\")            \n",
    "csv_time_dim.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"time_dim\",keyspace=\"tpcds\").mode(\"append\").save()   \n",
    "\n",
    "\n",
    "print(\"csv_ship_mode\")\n",
    "\n",
    "schema_ship_mode= StructType([StructField(\"sm_ship_mode_sk\",IntegerType(),True),StructField(\"sm_ship_mode_id\",StringType(),True),StructField(\"sm_type\",StringType(),True),StructField(\"sm_code\",StringType(),True),StructField(\"sm_carrier\",StringType(),True),StructField(\"sm_contract\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_ship_mode = spark.read.format(\"parquet\").schema(schema_ship_mode).load(\"./data/data_parquet/ship_mode/\", delimiter=\"|\")            \n",
    "csv_ship_mode.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"ship_mode\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "\n",
    "print(\"schema_promotion\")\n",
    "schema_promotion= StructType([StructField(\"p_promo_sk\",IntegerType(),True),StructField(\"p_promo_id\",StringType(),True),StructField(\"p_start_date_sk\",IntegerType(),True),StructField(\"p_end_date_sk\",IntegerType(),True),StructField(\"p_item_sk\",IntegerType(),True),StructField(\"p_cost\",FloatType(),True),StructField(\"p_response_target\",IntegerType(),True),StructField(\"p_promo_name\",StringType(),True),StructField(\"p_channel_dmail\",StringType(),True),StructField(\"p_channel_email\",StringType(),True),StructField(\"p_channel_catalog\",StringType(),True),StructField(\"p_channel_tv\",StringType(),True),StructField(\"p_channel_radio\",StringType(),True),StructField(\"p_channel_press\",StringType(),True),StructField(\"p_channel_event\",StringType(),True),StructField(\"p_channel_demo\",StringType(),True),StructField(\"p_channel_details\",StringType(),True),StructField(\"p_purpose\",StringType(),True),StructField(\"p_discount_active\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_promotion = spark.read.format(\"parquet\").schema(schema_promotion).load(\"./data/data_parquet/promotion/\", delimiter=\"|\")            \n",
    "csv_promotion.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"promotion\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "\n",
    "print(\"schema_reason\")\n",
    "\n",
    "schema_reason= StructType([StructField(\"r_reason_sk\",IntegerType(),True),StructField(\"r_reason_id\",StringType(),True),StructField(\"r_reason_desc\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_reason = spark.read.format(\"parquet\").schema(schema_reason).load(\"./data/data_parquet/reason/\", delimiter=\"|\")            \n",
    "csv_reason.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"reason\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "print(\"schema_item\")\n",
    "\n",
    "\n",
    "schema_item= StructType([StructField(\"i_item_sk\",IntegerType(),True),StructField(\"i_item_id\",StringType(),True),StructField(\"i_rec_start_date\",StringType(),True),StructField(\"i_rec_end_date\",StringType(),True),StructField(\"i_item_desc\",StringType(),True),StructField(\"i_current_price\",FloatType(),True),StructField(\"i_wholesale_cost\",FloatType(),True),StructField(\"i_brand_id\",IntegerType(),True),StructField(\"i_brand\",StringType(),True),StructField(\"i_class_id\",IntegerType(),True),StructField(\"i_class\",StringType(),True),StructField(\"i_category_id\",IntegerType(),True),StructField(\"i_category\",StringType(),True),StructField(\"i_manufact_id\",IntegerType(),True),StructField(\"i_manufact\",StringType(),True),StructField(\"i_size\",StringType(),True),StructField(\"i_formulation\",StringType(),True),StructField(\"i_color\",StringType(),True),StructField(\"i_units\",StringType(),True),StructField(\"i_container\",StringType(),True),StructField(\"i_manager_id\",IntegerType(),True),StructField(\"i_product_name\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_item = spark.read.format(\"parquet\").schema(schema_item).load(\"./data/data_parquet/item/\", delimiter=\"|\")            \n",
    "csv_item.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"item\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"schema_date_dim\")\n",
    "\n",
    "schema_date_dim= StructType([StructField(\"d_date_sk\",IntegerType(),True),StructField(\"d_date_id\",StringType(),True),StructField(\"d_date\",StringType(),True),StructField(\"d_month_seq\",IntegerType(),True),StructField(\"d_week_seq\",IntegerType(),True),StructField(\"d_quarter_seq\",IntegerType(),True),StructField(\"d_year\",IntegerType(),True),StructField(\"d_dow\",IntegerType(),True),StructField(\"d_moy\",IntegerType(),True),StructField(\"d_dom\",IntegerType(),True),StructField(\"d_qoy\",IntegerType(),True),StructField(\"d_fy_year\",IntegerType(),True),StructField(\"d_fy_quarter_seq\",IntegerType(),True),StructField(\"d_fy_week_seq\",IntegerType(),True),StructField(\"d_day_name\",StringType(),True),StructField(\"d_quarter_name\",StringType(),True),StructField(\"d_holiday\",StringType(),True),StructField(\"d_weekend\",StringType(),True),StructField(\"d_following_holiday\",StringType(),True),StructField(\"d_first_dom\",IntegerType(),True),StructField(\"d_last_dom\",IntegerType(),True),StructField(\"d_same_day_ly\",IntegerType(),True),StructField(\"d_same_day_lq\",IntegerType(),True),StructField(\"d_current_day\",StringType(),True),StructField(\"d_current_week\",StringType(),True),StructField(\"d_current_month\",StringType(),True),StructField(\"d_current_quarter\",StringType(),True),StructField(\"d_current_year\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_date_dim = spark.read.format(\"parquet\").schema(schema_date_dim).load(\"./data/data_parquet/date_dim/\", delimiter=\"|\")            \n",
    "csv_date_dim.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"date_dim\",keyspace=\"tpcds\").mode(\"append\").save() \n",
    "\n",
    "print(\"schema_household_demographics\")\n",
    "\n",
    "schema_household_demographics= StructType([StructField(\"hd_demo_sk\",IntegerType(),True),StructField(\"hd_income_band_sk\",IntegerType(),True),StructField(\"hd_buy_potential\",StringType(),True),StructField(\"hd_dep_count\",IntegerType(),True),StructField(\"hd_vehicle_count\",IntegerType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_household_demographics = spark.read.format(\"parquet\").schema(schema_household_demographics).load(\"./data/data_parquet/household_demographics/\", delimiter=\"|\")            \n",
    "csv_household_demographics.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"household_demographics\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "\n",
    "print(\"schema_income_band\")\n",
    "\n",
    "schema_income_band= StructType([StructField(\"ib_income_band_sk\",IntegerType(),True),StructField(\"ib_lower_bound\",IntegerType(),True),StructField(\"ib_upper_bound\",IntegerType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_income_band = spark.read.format(\"parquet\").schema(schema_income_band).load(\"./data/data_parquet/income_band/\", delimiter=\"|\")            \n",
    "csv_income_band.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"income_band\",keyspace=\"tpcds\").mode(\"append\").save() \n",
    "\n",
    "\n",
    "## REAL TIME TABLE TO BE USED WITH NOSQLBENCH\n",
    "# schema_web_sales = StructType([StructField(\"ws_sold_date_sk\",IntegerType(),True),StructField(\"ws_sold_time_sk\",IntegerType(),True),StructField(\"ws_ship_date_sk\",IntegerType(),True),StructField(\"ws_item_sk\",IntegerType(),True),StructField(\"ws_bill_customer_sk\",IntegerType(),True),StructField(\"ws_bill_cdemo_sk\",IntegerType(),True),StructField(\"ws_bill_hdemo_sk\",IntegerType(),True),StructField(\"ws_bill_addr_sk\",IntegerType(),True),StructField(\"ws_ship_customer_sk\",IntegerType(),True),StructField(\"ws_ship_cdemo_sk\",IntegerType(),True),StructField(\"ws_ship_hdemo_sk\",IntegerType(),True),StructField(\"ws_ship_addr_sk\",IntegerType(),True),StructField(\"ws_web_page_sk\",IntegerType(),True),StructField(\"ws_web_site_sk\",IntegerType(),True),StructField(\"ws_ship_mode_sk\",IntegerType(),True),StructField(\"ws_warehouse_sk\",IntegerType(),True),StructField(\"ws_promo_sk\",IntegerType(),True),StructField(\"ws_order_number\",IntegerType(),True),StructField(\"ws_quantity\",IntegerType(),True),StructField(\"ws_wholesale_cost\",FloatType(),True),StructField(\"ws_list_price\",FloatType(),True),StructField(\"ws_sales_price\",FloatType(),True),StructField(\"ws_ext_discount_amt\",FloatType(),True),StructField(\"ws_ext_sales_price\",FloatType(),True),StructField(\"ws_ext_wholesale_cost\",FloatType(),True),StructField(\"ws_ext_list_price\",FloatType(),True),StructField(\"ws_ext_tax\",FloatType(),True),StructField(\"ws_coupon_amt\",FloatType(),True),StructField(\"ws_ext_ship_cost\",FloatType(),True),StructField(\"ws_net_paid\",FloatType(),True),StructField(\"ws_net_paid_inc_tax\",FloatType(),True),StructField(\"ws_net_paid_inc_ship\",FloatType(),True),StructField(\"ws_net_paid_inc_ship_tax\",FloatType(),True),StructField(\"ws_net_profit\",FloatType(),True),StructField(\"junk\",StringType(),True),])\n",
    "# csv_web_sales = spark.read.format(\"parquet\").schema(schema_web_sales).load(\"/home/ricardo/tpcds-kit/./data/data_parquet/out/web_sales/\", delimiter=\"|\")\n",
    "# csv_web_sales.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_sales\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "# print(\"catalog_sales\")\n",
    "# catalog_sales_schema = StructType([ StructField(\"cs_sold_date_sk\",IntegerType(),True), StructField(\"cs_sold_time_sk\",IntegerType(),True), StructField(\"cs_ship_date_sk\",IntegerType(),True), StructField(\"cs_bill_customer_sk\",IntegerType(),True), StructField(\"cs_bill_cdemo_sk\",IntegerType(),True), StructField(\"cs_bill_hdemo_sk\",IntegerType(),True), StructField(\"cs_bill_addr_sk\",IntegerType(),True), StructField(\"cs_ship_customer_sk\",IntegerType(),True), StructField(\"cs_ship_cdemo_sk\",IntegerType(),True), StructField(\"cs_ship_hdemo_sk\",IntegerType(),True), StructField(\"cs_ship_addr_sk\",IntegerType(),True), StructField(\"cs_call_center_sk\",IntegerType(),True), StructField(\"cs_catalog_page_sk\",IntegerType(),True), StructField(\"cs_ship_mode_sk\",IntegerType(),True), StructField(\"cs_warehouse_sk\",IntegerType(),True), StructField(\"cs_item_sk\",IntegerType(),True), StructField(\"cs_promo_sk\",IntegerType(),True), StructField(\"cs_order_number\",IntegerType(),True), StructField(\"cs_quantity\",IntegerType(),True), StructField(\"cs_wholesale_cost\",FloatType(),True), StructField(\"cs_list_price\",FloatType(),True), StructField(\"cs_sales_price\",FloatType(),True), StructField(\"cs_ext_discount_amt\",FloatType(),True), StructField(\"cs_ext_sales_price\",FloatType(),True), StructField(\"cs_ext_wholesale_cost\",FloatType(),True), StructField(\"cs_ext_list_price\",FloatType(),True), StructField(\"cs_ext_tax\",FloatType(),True), StructField(\"cs_coupon_amt\",FloatType(),True), StructField(\"cs_ext_ship_cost\",FloatType(),True), StructField(\"cs_net_paid\",FloatType(),True), StructField(\"cs_net_paid_inc_tax\",FloatType(),True), StructField(\"cs_net_paid_inc_ship\",FloatType(),True), StructField(\"cs_net_paid_inc_ship_tax\",FloatType(),True), StructField(\"cs_net_profit\",FloatType(),True), StructField(\"junk\",StringType(),True), ])\n",
    "# csv_catalog_sales = spark.read.format(\"parquet\").schema(catalog_sales_schema).load(\"./data/data_parquet/catalog_sales/\", delimiter=\"|\")\n",
    "# csv_catalog_sales.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"catalog_sales\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "\n",
    "# print(\"schema_store_returns\")\n",
    "# schema_store_returns = StructType([ StructField(\"sr_returned_date_sk\",IntegerType(),True), StructField(\"sr_return_time_sk\",IntegerType(),True), StructField(\"sr_item_sk\",IntegerType(),True), StructField(\"sr_customer_sk\",IntegerType(),True), StructField(\"sr_cdemo_sk\",IntegerType(),True), StructField(\"sr_hdemo_sk\",IntegerType(),True), StructField(\"sr_addr_sk\",IntegerType(),True), StructField(\"sr_store_sk\",IntegerType(),True), StructField(\"sr_reason_sk\",IntegerType(),True), StructField(\"sr_ticket_number\",IntegerType(),True), StructField(\"sr_return_quantity\",IntegerType(),True), StructField(\"sr_return_amt\",FloatType(),True), StructField(\"sr_return_tax\",FloatType(),True), StructField(\"sr_return_amt_inc_tax\",FloatType(),True), StructField(\"sr_fee\",FloatType(),True), StructField(\"sr_return_ship_cost\",FloatType(),True), StructField(\"sr_refunded_cash\",FloatType(),True), StructField(\"sr_reversed_charge\",FloatType(),True), StructField(\"sr_store_credit\",FloatType(),True), StructField(\"sr_net_loss\",FloatType(),True), StructField(\"junk\",StringType(),True)])\n",
    "# csv_store_returns = spark.read.format(\"parquet\").schema(schema_store_returns).load(\"./data/data_parquet/store_returns/\", delimiter=\"|\")\n",
    "# csv_store_returns.write .format(\"org.apache.spark.sql.cassandra\") .options(table=\"store_returns\",keyspace=\"tpcds\") .mode(\"append\").save()   \n",
    "\n",
    "\n",
    "# print(\"schema_web_returns\")\n",
    "# schema_web_returns = StructType([StructField(\"wr_returned_date_sk\",IntegerType(),True),StructField(\"wr_returned_time_sk\",IntegerType(),True),StructField(\"wr_item_sk\",IntegerType(),True),StructField(\"wr_refunded_customer_sk\",IntegerType(),True),StructField(\"wr_refunded_cdemo_sk\",IntegerType(),True),StructField(\"wr_refunded_hdemo_sk\",IntegerType(),True),StructField(\"wr_refunded_addr_sk\",IntegerType(),True),StructField(\"wr_returning_customer_sk\",IntegerType(),True),StructField(\"wr_returning_cdemo_sk\",IntegerType(),True),StructField(\"wr_returning_hdemo_sk\",IntegerType(),True),StructField(\"wr_returning_addr_sk\",IntegerType(),True),StructField(\"wr_web_page_sk\",IntegerType(),True),StructField(\"wr_reason_sk\",IntegerType(),True),StructField(\"wr_order_number\",IntegerType(),True),StructField(\"wr_return_quantity\",IntegerType(),True),StructField(\"wr_return_amt\",FloatType(),True),StructField(\"wr_return_tax\",FloatType(),True),StructField(\"wr_return_amt_inc_tax\",FloatType(),True),StructField(\"wr_fee\",FloatType(),True),StructField(\"wr_return_ship_cost\",FloatType(),True),StructField(\"wr_refunded_cash\",FloatType(),True),StructField(\"wr_reversed_charge\",FloatType(),True),StructField(\"wr_account_credit\",FloatType(),True),StructField(\"wr_net_loss\",FloatType(),True),StructField(\"junk\",StringType(),True),])\n",
    "# csv_web_returns = spark.read.format(\"parquet\").schema(schema_web_returns).load(\"/home/ricardo/tpcds-kit/./data/data_parquet/out/web_returns/\", delimiter=\"|\")\n",
    "# csv_web_returns.write .format(\"org.apache.spark.sql.cassandra\") .options(table=\"web_returns\",keyspace=\"tpcds\") .mode(\"append\").save()\n",
    "\n",
    "# print(\"store_sales\")\n",
    "# schema_store_sales = StructType([ StructField(\"ss_sold_date_sk\",IntegerType(),True), StructField(\"ss_sold_time_sk\",IntegerType(),True), StructField(\"ss_item_sk\",IntegerType(),True), StructField(\"ss_customer_sk\",IntegerType(),True), StructField(\"ss_cdemo_sk\",IntegerType(),True), StructField(\"ss_hdemo_sk\",IntegerType(),True), StructField(\"ss_addr_sk\",IntegerType(),True), StructField(\"ss_store_sk\",IntegerType(),True), StructField(\"ss_promo_sk\",IntegerType(),True), StructField(\"ss_ticket_number\",IntegerType(),True), StructField(\"ss_quantity\",IntegerType(),True), StructField(\"ss_wholesale_cost\",FloatType(),True), StructField(\"ss_list_price\",FloatType(),True), StructField(\"ss_sales_price\",FloatType(),True), StructField(\"ss_ext_discount_amt\",FloatType(),True), StructField(\"ss_ext_sales_price\",FloatType(),True), StructField(\"ss_ext_wholesale_cost\",FloatType(),True), StructField(\"ss_ext_list_price\",FloatType(),True), StructField(\"ss_ext_tax\",FloatType(),True), StructField(\"ss_coupon_amt\",FloatType(),True), StructField(\"ss_net_paid\",FloatType(),True), StructField(\"ss_net_paid_inc_tax\",FloatType(),True), StructField(\"ss_net_profit\",FloatType(),True), StructField(\"junk\",StringType(),True),  ])\n",
    "# csv_store_sales = spark.read.format(\"parquet\").schema(schema_store_sales).load(\"./data/data_parquet/store_sales/\", delimiter=\"|\")\n",
    "# csv_store_sales.write .format(\"org.apache.spark.sql.cassandra\") .options(table=\"store_sales\",keyspace=\"tpcds\") .mode(\"append\").save() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Real-time Workload with Real-time-Workload Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before proceeding, run the Real-time-Workload notebook first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Analytical SQL Queries with SparkSQL and Scylla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "IPS = ['172.19.0.2']\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"IoT-Scylla\")\\\n",
    "    .config(\"setMaster\",\"local[*]\")\\\n",
    "    .config(\"spark.jars\", \"target/scala-2.12/spark3-scylla4-example-assembly-0.1.jar\")\\\n",
    "    .config(\"spark.cassandra.connection.host\", ','.join(IPS))\\\n",
    "    .config('spark.cassandra.concurrent.reads','20480')\\\n",
    "    .config('spark.cassandra.input.consistency.level','LOCAL_ONE')\\\n",
    "    .config('spark.cassandra.input.fetch.sizeInRows','2000')\\\n",
    "    .config('spark.cassandra.input.split.sizeInMB','512')\\\n",
    "    .config(\"spark.driver.memory\", \"5g\")\\\n",
    "    .config(\"spark.executor.memory\", \"5g\")\\\n",
    "    .config(\"spark.driver.cores\",5)\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "sc._conf.get('spark.executor.memory')\n",
    "sc._conf.get('spark.driver.memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Registering Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_center = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"call_center\", keyspace=\"tpcds\").load()\n",
    "call_center.registerTempTable(\"call_center\")\n",
    "catalog_page = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"catalog_page\", keyspace=\"tpcds\").load()\n",
    "catalog_page.registerTempTable(\"catalog_page\")\n",
    "catalog_returns = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"catalog_returns\", keyspace=\"tpcds\").load()\n",
    "catalog_returns.registerTempTable(\"catalog_returns\")\n",
    "catalog_sales = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"catalog_sales\", keyspace=\"tpcds\").load()\n",
    "catalog_sales.registerTempTable(\"catalog_sales\")\n",
    "customer = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer\", keyspace=\"tpcds\").load()\n",
    "customer.registerTempTable(\"customer\")\n",
    "customer_address = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_address\", keyspace=\"tpcds\").load()\n",
    "customer_address.registerTempTable(\"customer_address\")\n",
    "customer_demographics = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_demographics\", keyspace=\"tpcds\").load()\n",
    "customer_demographics.registerTempTable(\"customer_demographics\")\n",
    "date_dim = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"date_dim\", keyspace=\"tpcds\").load()\n",
    "date_dim.registerTempTable(\"date_dim\")\n",
    "household_demographics = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"household_demographics\", keyspace=\"tpcds\").load()\n",
    "household_demographics.registerTempTable(\"household_demographics\")\n",
    "income_band = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"income_band\", keyspace=\"tpcds\").load()\n",
    "income_band.registerTempTable(\"income_band\")\n",
    "inventory = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"inventory\", keyspace=\"tpcds\").load()\n",
    "inventory.registerTempTable(\"inventory\")\n",
    "item = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"item\", keyspace=\"tpcds\").load()\n",
    "item.registerTempTable(\"item\")\n",
    "promotion = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"promotion\", keyspace=\"tpcds\").load()\n",
    "promotion.registerTempTable(\"promotion\")\n",
    "reason = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"reason\", keyspace=\"tpcds\").load()\n",
    "reason.registerTempTable(\"reason\")\n",
    "ship_mode = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"ship_mode\", keyspace=\"tpcds\").load()\n",
    "ship_mode.registerTempTable(\"ship_mode\")\n",
    "store = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"store\", keyspace=\"tpcds\").load()\n",
    "store.registerTempTable(\"store\")\n",
    "store_returns = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"store_returns\", keyspace=\"tpcds\").load()\n",
    "store_returns.registerTempTable(\"store_returns\")\n",
    "store_sales = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"store_sales\", keyspace=\"tpcds\").load()\n",
    "store_sales.registerTempTable(\"store_sales\")\n",
    "time_dim = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"time_dim\", keyspace=\"tpcds\").load()\n",
    "time_dim.registerTempTable(\"time_dim\")\n",
    "warehouse = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"warehouse\", keyspace=\"tpcds\").load()\n",
    "warehouse.registerTempTable(\"warehouse\")\n",
    "web_page = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_page\", keyspace=\"tpcds\").load()\n",
    "web_page.registerTempTable(\"web_page\")\n",
    "web_returns = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_returns\", keyspace=\"tpcds\").load()\n",
    "web_returns.registerTempTable(\"web_returns\")\n",
    "web_sales = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_sales\", keyspace=\"tpcds\").load()\n",
    "web_sales.registerTempTable(\"web_sales\")\n",
    "web_site = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_site\", keyspace=\"tpcds\").load()\n",
    "web_site.registerTempTable(\"web_site\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = '''select d_date from web_sales join date_dim on ws_sold_date_sk = d_date_sk\n",
    "                              where d_date between \\'2021-06-20' and \\'2021-06-21\\''''\n",
    "sqlContext.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query1= '''select \n",
    "    i_item_desc,\n",
    "  i_category,\n",
    "  i_class,\n",
    "  i_current_price,\n",
    "  i_item_id,\n",
    "  sum(ws_ext_sales_price) as itemrevenue\n",
    "from\n",
    "  web_sales\n",
    "  join item on (web_sales.ws_item_sk = item.i_item_sk)\n",
    "where\n",
    "  i_category in('Jewelry', 'Sports', 'Books')\n",
    "  and ws_sold_date_sk = 2459247\n",
    "group by\n",
    "  i_item_id,\n",
    "  i_item_desc,\n",
    "  i_category,\n",
    "  i_class,\n",
    "  i_current_price\n",
    "order by\n",
    "  i_category,\n",
    "  i_class,\n",
    "  i_item_id,\n",
    "  i_item_desc\n",
    "  -- revenueratio\n",
    "limit 1000;'''\n",
    "\n",
    "sqlContext.sql(query1).show(100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
