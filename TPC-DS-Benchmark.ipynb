{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail  Real-Time Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing library\n",
    "! pip3 install scylla-driver ipyparallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster, ExecutionProfile, EXEC_PROFILE_DEFAULT\n",
    "from cassandra.policies import DCAwareRoundRobinPolicy, TokenAwarePolicy, DowngradingConsistencyRetryPolicy, ConsistencyLevel, RoundRobinPolicy\n",
    "from cassandra.query import tuple_factory\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType, DateType, LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set your Scylla IP(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPS=['172.19.0.2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the execution profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ExecutionProfile(\n",
    "    load_balancing_policy=TokenAwarePolicy(DCAwareRoundRobinPolicy(\"datacenter1\")),\n",
    "    retry_policy=DowngradingConsistencyRetryPolicy(), ##CHECK \n",
    "    consistency_level=ConsistencyLevel.LOCAL_QUORUM,\n",
    "    serial_consistency_level=ConsistencyLevel.LOCAL_SERIAL, ##CHECK LWT \n",
    "    request_timeout=20000,\n",
    "    row_factory=tuple_factory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(IPS,execution_profiles={EXEC_PROFILE_DEFAULT: profile})\n",
    "session = cluster.connect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace and connect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_keyspace = \"CREATE KEYSPACE IF NOT EXISTS tpcds WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1' : '1'};\"\n",
    "#session = cluster.connect()\n",
    "session.execute(create_keyspace)\n",
    "session = cluster.connect('tpcds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the tables from the CQL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('ddl.cql', mode='r') as f:\n",
    "    txt = f.read()\n",
    "    stmts = txt.split(r';')\n",
    "    for i in stmts:\n",
    "        stmt = i.strip()\n",
    "        if stmt != '':\n",
    "            print('Executing: \"' + stmt + '\"')\n",
    "            session.execute(stmt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data for Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN ON YOUR HOST, NOT ON THE DOCKER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt-get install gcc make flex bison byacc git\n",
    "git clone https://github.com/gregrahn/tpcds-kit\n",
    "cd /home/$USER/tpcds-kit/tools/ \n",
    "make OS=LINUX \n",
    "mv /home/$USER/tpcds-kit/tools/ /home/$USER/scylla-code-samples/spark3-scylla4-demo/tools/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating CSV using DSDGEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsdgen Population Generator (Version 2.10.0)\r\n",
      "Copyright Transaction Processing Performance Council (TPC) 2001 - 2018\r\n",
      "Warning: This scale factor is valid for QUALIFICATION ONLY\r\n",
      "ERROR: ./call_center.dat exists. Either remove it or use the FORCE option to overwrite it.\r\n"
     ]
    }
   ],
   "source": [
    "!cd tools/ && ./dsdgen -sc 1 && mv ./tools/dsdgen/*.dat /home/jovyan/work/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading using Spark\n",
    "#### Spark Integration and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"IoT-Scylla\")\\\n",
    "    .config(\"setMaster\",\"local[*]\")\\\n",
    "    .config(\"spark.jars\", \"target/scala-2.12/spark3-scylla4-example-assembly-0.1.jar\")\\\n",
    "    .config(\"spark.cassandra.connection.host\", ','.join(IPS))\\\n",
    "    .config('spark.cassandra.concurrent.reads','512')\\\n",
    "    .config('spark.cassandra.input.consistency.level','LOCAL_ONE')\\\n",
    "    .config('spark.cassandra.input.fetch.sizeInRows','1000')\\\n",
    "    .config('spark.cassandra.input.split.sizeInMB','512')\\\n",
    "    .config(\"spark.driver.memory\", \"5g\")\\\n",
    "    .config(\"spark.executor.memory\", \"5g\")\\\n",
    "    .config(\"spark.driver.cores\",5)\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining schemas and writting the CSV into Scylla using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"store\")\n",
    "\n",
    "schema_store= StructType([StructField(\"s_store_sk\",IntegerType(),True),StructField(\"s_store_id\",StringType(),True),StructField(\"s_rec_start_date\",StringType(),True),StructField(\"s_rec_end_date\",StringType(),True),StructField(\"s_closed_date_sk\",IntegerType(),True),StructField(\"s_store_name\",StringType(),True),StructField(\"s_number_employees\",IntegerType(),True),StructField(\"s_floor_space\",IntegerType(),True),StructField(\"s_hours\",StringType(),True),StructField(\"s_manager\",StringType(),True),StructField(\"s_market_id\",IntegerType(),True),StructField(\"s_geography_class\",StringType(),True),StructField(\"s_market_desc\",StringType(),True),StructField(\"s_market_manager\",StringType(),True),StructField(\"s_division_id\",IntegerType(),True),StructField(\"s_division_name\",StringType(),True),StructField(\"s_company_id\",IntegerType(),True),StructField(\"s_company_name\",StringType(),True),StructField(\"s_street_number\",StringType(),True),StructField(\"s_street_name\",StringType(),True),StructField(\"s_street_type\",StringType(),True),StructField(\"s_suite_number\",StringType(),True),StructField(\"s_city\",StringType(),True),StructField(\"s_county\",StringType(),True),StructField(\"s_state\",StringType(),True),StructField(\"s_zip\",StringType(),True),StructField(\"s_country\",StringType(),True),StructField(\"s_gmt_offset\",FloatType(),True),StructField(\"s_tax_precentage\",FloatType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_store = spark.read.format(\"csv\").schema(schema_store).load(\"./work/data/store.dat\", delimiter=\"|\")            \n",
    "csv_store.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"store\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "\n",
    "\n",
    "print(\"customer_address\")\n",
    "schema_customer_address = StructType([ StructField(\"ca_address_sk\",IntegerType(),True), StructField(\"ca_address_id\",StringType(),True), StructField(\"ca_street_number\",StringType(),True), StructField(\"ca_street_name\",StringType(),True), StructField(\"ca_street_type\",StringType(),True), StructField(\"ca_suite_number\",StringType(),True), StructField(\"ca_city\",StringType(),True), StructField(\"ca_county\",StringType(),True), StructField(\"ca_state\",StringType(),True), StructField(\"ca_zip\",StringType(),True), StructField(\"ca_country\",StringType(),True), StructField(\"ca_gmt_offset\",FloatType(),True), StructField(\"ca_location_type\",StringType(),True), StructField(\"junk\",StringType(),True), ])\n",
    "csv_customer_address = spark.read.format(\"csv\").schema(schema_customer_address).load(\"./work/data/customer_address*.dat\", delimiter=\"|\")\n",
    "csv_customer_address.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_address\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "\n",
    "\n",
    "print(\"customer\")\n",
    "schema_customer = StructType([ StructField(\"c_customer_sk\",IntegerType(),True), StructField(\"c_customer_id\",StringType(),True), StructField(\"c_current_cdemo_sk\",IntegerType(),True), StructField(\"c_current_hdemo_sk\",IntegerType(),True), StructField(\"c_current_addr_sk\",IntegerType(),True), StructField(\"c_first_shipto_date_sk\",IntegerType(),True), StructField(\"c_first_sales_date_sk\",IntegerType(),True), StructField(\"c_salutation\",StringType(),True), StructField(\"c_first_name\",StringType(),True), StructField(\"c_last_name\",StringType(),True), StructField(\"c_preferred_cust_flag\",StringType(),True), StructField(\"c_birth_day\",IntegerType(),True), StructField(\"c_birth_month\",IntegerType(),True), StructField(\"c_birth_year\",IntegerType(),True), StructField(\"c_birth_country\",StringType(),True), StructField(\"c_login\",StringType(),True), StructField(\"c_email_address\",StringType(),True), StructField(\"c_last_review_date\",StringType(),True), StructField(\"junk\",StringType(),True) ])\n",
    "csv_customer = spark.read.format(\"csv\").schema(schema_customer).load(\"./work/data/customer*.dat\", delimiter=\"|\")\n",
    "csv_customer.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"customer_demo\")\n",
    "schema_customer_demo = StructType([ StructField(\"cd_demo_sk\",IntegerType(),True), StructField(\"cd_gender\",StringType(),True), StructField(\"cd_marital_status\",StringType(),True), StructField(\"cd_education_status\",StringType(),True), StructField(\"cd_purchase_estimate\",IntegerType(),True), StructField(\"cd_credit_rating\",StringType(),True), StructField(\"cd_dep_count\",IntegerType(),True), StructField(\"cd_dep_employed_count\",IntegerType(),True), StructField(\"cd_dep_college_count\",IntegerType(),True), StructField(\"junk\",StringType(),True), ])\n",
    "csv_customer_demo = spark.read.format(\"csv\").schema(schema_customer_demo).load(\"./work/data/customer_demographics*.dat\", delimiter=\"|\")\n",
    "csv_customer_demo.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_demographics\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "schema_inventory = StructType([ StructField(\"inv_date_sk\",IntegerType(),True), StructField(\"inv_item_sk\",IntegerType(),True), StructField(\"inv_warehouse_sk\",IntegerType(),True), StructField(\"inv_quantity_on_hand\",IntegerType(),True), StructField(\"junk\",IntegerType(),True),  ])\n",
    "csv_inventory = spark.read.format(\"csv\").schema(schema_inventory).load(\"./work/data/inventory*.dat\", delimiter=\"|\")\n",
    "csv_inventory.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"inventory\",keyspace=\"tpcds\").mode(\"append\").save()   \n",
    "\n",
    "\n",
    "print(\"schema_web_site\")\n",
    "\n",
    "schema_web_site = StructType([StructField(\"web_site_sk\",IntegerType(),True),StructField(\"web_site_id\",StringType(),True),StructField(\"web_rec_start_date\",StringType(),True),StructField(\"web_rec_end_date\",StringType(),True),StructField(\"web_name\",StringType(),True),StructField(\"web_open_date_sk\",IntegerType(),True),StructField(\"web_close_date_sk\",IntegerType(),True),StructField(\"web_class\",StringType(),True),StructField(\"web_manager\",StringType(),True),StructField(\"web_mkt_id\",IntegerType(),True),StructField(\"web_mkt_class\",StringType(),True),StructField(\"web_mkt_desc\",StringType(),True),StructField(\"web_market_manager\",StringType(),True),StructField(\"web_company_id\",IntegerType(),True),StructField(\"web_company_name\",StringType(),True),StructField(\"web_street_number\",StringType(),True),StructField(\"web_street_name\",StringType(),True),StructField(\"web_street_type\",StringType(),True),StructField(\"web_suite_number\",StringType(),True),StructField(\"web_city\",StringType(),True),StructField(\"web_county\",StringType(),True),StructField(\"web_state\",StringType(),True),StructField(\"web_zip\",StringType(),True),StructField(\"web_country\",StringType(),True),StructField(\"web_gmt_offset\",FloatType(),True),StructField(\"web_tax_percentage\",FloatType(),True),StructField(\"junk\",StringType(),True),   ])  \n",
    "csv_web_site = spark.read.format(\"csv\").schema(schema_web_site).load(\"./work/data/web_site*.dat\", delimiter=\"|\")            \n",
    "csv_web_site.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_site\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"schema_web_page\")\n",
    "\n",
    "schema_web_page = StructType([StructField(\"wp_web_page_sk\",IntegerType(),True),StructField(\"wp_web_page_id\",StringType(),True),StructField(\"wp_rec_start_date\",StringType(),True),StructField(\"wp_rec_end_date\",StringType(),True),StructField(\"wp_creation_date_sk\",IntegerType(),True),StructField(\"wp_access_date_sk\",IntegerType(),True),StructField(\"wp_autogen_flag\",StringType(),True),StructField(\"wp_customer_sk\",IntegerType(),True),StructField(\"wp_url\",StringType(),True),StructField(\"wp_type\",StringType(),True),StructField(\"wp_char_count\",IntegerType(),True),StructField(\"wp_link_count\",IntegerType(),True),StructField(\"wp_image_count\",IntegerType(),True),StructField(\"wp_max_ad_count\",IntegerType(),True),StructField(\"junk\",StringType(),True),])  \n",
    "csv_web_page = spark.read.format(\"csv\").schema(schema_web_page).load(\"./work/data/web_page*.dat\", delimiter=\"|\")            \n",
    "csv_web_page.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_page\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"schema_warehouse\")\n",
    "\n",
    "schema_warehouse = StructType([StructField(\"w_warehouse_sk\",IntegerType(),True),StructField(\"w_warehouse_id\",StringType(),True),StructField(\"w_warehouse_name\",StringType(),True),StructField(\"w_warehouse_sq_ft\",IntegerType(),True),StructField(\"w_street_number\",StringType(),True),StructField(\"w_street_name\",StringType(),True),StructField(\"w_street_type\",StringType(),True),StructField(\"w_suite_number\",StringType(),True),StructField(\"w_city\",StringType(),True),StructField(\"w_county\",StringType(),True),StructField(\"w_state\",StringType(),True),StructField(\"w_zip\",StringType(),True),StructField(\"w_country\",StringType(),True),StructField(\"w_gmt_offset\",FloatType(),True),StructField(\"junk\",StringType(),True), ])            \n",
    "csv_warehouse = spark.read.format(\"csv\").schema(schema_warehouse).load(\"./work/data/warehouse*.dat\", delimiter=\"|\")            \n",
    "csv_warehouse.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"warehouse\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"schema_time\")\n",
    "\n",
    "schema_time= StructType([StructField(\"t_time_sk\",IntegerType(),True),StructField(\"t_time_id\",StringType(),True),StructField(\"t_time\",IntegerType(),True),StructField(\"t_hour\",IntegerType(),True),StructField(\"t_minute\",IntegerType(),True),StructField(\"t_second\",IntegerType(),True),StructField(\"t_am_pm\",StringType(),True),StructField(\"t_shift\",StringType(),True),StructField(\"t_sub_shift\",StringType(),True),StructField(\"t_meal_time\",StringType(),True),StructField(\"junk\",StringType(),True), ])            \n",
    "csv_time_dim = spark.read.format(\"csv\").schema(schema_time).load(\"./work/data/time_dim*.dat\", delimiter=\"|\")            \n",
    "csv_time_dim.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"time_dim\",keyspace=\"tpcds\").mode(\"append\").save()   \n",
    "\n",
    "\n",
    "print(\"csv_ship_mode\")\n",
    "\n",
    "schema_ship_mode= StructType([StructField(\"sm_ship_mode_sk\",IntegerType(),True),StructField(\"sm_ship_mode_id\",StringType(),True),StructField(\"sm_type\",StringType(),True),StructField(\"sm_code\",StringType(),True),StructField(\"sm_carrier\",StringType(),True),StructField(\"sm_contract\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_ship_mode = spark.read.format(\"csv\").schema(schema_ship_mode).load(\"./work/data/ship_mode*.dat\", delimiter=\"|\")            \n",
    "csv_ship_mode.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"ship_mode\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "\n",
    "print(\"schema_promotion\")\n",
    "schema_promotion= StructType([StructField(\"p_promo_sk\",IntegerType(),True),StructField(\"p_promo_id\",StringType(),True),StructField(\"p_start_date_sk\",IntegerType(),True),StructField(\"p_end_date_sk\",IntegerType(),True),StructField(\"p_item_sk\",IntegerType(),True),StructField(\"p_cost\",FloatType(),True),StructField(\"p_response_target\",IntegerType(),True),StructField(\"p_promo_name\",StringType(),True),StructField(\"p_channel_dmail\",StringType(),True),StructField(\"p_channel_email\",StringType(),True),StructField(\"p_channel_catalog\",StringType(),True),StructField(\"p_channel_tv\",StringType(),True),StructField(\"p_channel_radio\",StringType(),True),StructField(\"p_channel_press\",StringType(),True),StructField(\"p_channel_event\",StringType(),True),StructField(\"p_channel_demo\",StringType(),True),StructField(\"p_channel_details\",StringType(),True),StructField(\"p_purpose\",StringType(),True),StructField(\"p_discount_active\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_promotion = spark.read.format(\"csv\").schema(schema_promotion).load(\"./work/data/promotion*.dat\", delimiter=\"|\")            \n",
    "csv_promotion.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"promotion\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "\n",
    "print(\"schema_reason\")\n",
    "\n",
    "schema_reason= StructType([StructField(\"r_reason_sk\",IntegerType(),True),StructField(\"r_reason_id\",StringType(),True),StructField(\"r_reason_desc\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_reason = spark.read.format(\"csv\").schema(schema_reason).load(\"./work/data/reason*.dat\", delimiter=\"|\")            \n",
    "csv_reason.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"reason\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "print(\"schema_item\")\n",
    "\n",
    "\n",
    "schema_item= StructType([StructField(\"i_item_sk\",IntegerType(),True),StructField(\"i_item_id\",StringType(),True),StructField(\"i_rec_start_date\",StringType(),True),StructField(\"i_rec_end_date\",StringType(),True),StructField(\"i_item_desc\",StringType(),True),StructField(\"i_current_price\",FloatType(),True),StructField(\"i_wholesale_cost\",FloatType(),True),StructField(\"i_brand_id\",IntegerType(),True),StructField(\"i_brand\",StringType(),True),StructField(\"i_class_id\",IntegerType(),True),StructField(\"i_class\",StringType(),True),StructField(\"i_category_id\",IntegerType(),True),StructField(\"i_category\",StringType(),True),StructField(\"i_manufact_id\",IntegerType(),True),StructField(\"i_manufact\",StringType(),True),StructField(\"i_size\",StringType(),True),StructField(\"i_formulation\",StringType(),True),StructField(\"i_color\",StringType(),True),StructField(\"i_units\",StringType(),True),StructField(\"i_container\",StringType(),True),StructField(\"i_manager_id\",IntegerType(),True),StructField(\"i_product_name\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_item = spark.read.format(\"csv\").schema(schema_item).load(\"./work/data/item*.dat\", delimiter=\"|\")            \n",
    "csv_item.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"item\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "print(\"schema_date_dim\")\n",
    "\n",
    "schema_date_dim= StructType([StructField(\"d_date_sk\",IntegerType(),True),StructField(\"d_date_id\",StringType(),True),StructField(\"d_date\",StringType(),True),StructField(\"d_month_seq\",IntegerType(),True),StructField(\"d_week_seq\",IntegerType(),True),StructField(\"d_quarter_seq\",IntegerType(),True),StructField(\"d_year\",IntegerType(),True),StructField(\"d_dow\",IntegerType(),True),StructField(\"d_moy\",IntegerType(),True),StructField(\"d_dom\",IntegerType(),True),StructField(\"d_qoy\",IntegerType(),True),StructField(\"d_fy_year\",IntegerType(),True),StructField(\"d_fy_quarter_seq\",IntegerType(),True),StructField(\"d_fy_week_seq\",IntegerType(),True),StructField(\"d_day_name\",StringType(),True),StructField(\"d_quarter_name\",StringType(),True),StructField(\"d_holiday\",StringType(),True),StructField(\"d_weekend\",StringType(),True),StructField(\"d_following_holiday\",StringType(),True),StructField(\"d_first_dom\",IntegerType(),True),StructField(\"d_last_dom\",IntegerType(),True),StructField(\"d_same_day_ly\",IntegerType(),True),StructField(\"d_same_day_lq\",IntegerType(),True),StructField(\"d_current_day\",StringType(),True),StructField(\"d_current_week\",StringType(),True),StructField(\"d_current_month\",StringType(),True),StructField(\"d_current_quarter\",StringType(),True),StructField(\"d_current_year\",StringType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_date_dim = spark.read.format(\"csv\").schema(schema_date_dim).load(\"./work/data/date_dim*.dat\", delimiter=\"|\")            \n",
    "csv_date_dim.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"date_dim\",keyspace=\"tpcds\").mode(\"append\").save() \n",
    "\n",
    "print(\"schema_household_demographics\")\n",
    "\n",
    "schema_household_demographics= StructType([StructField(\"hd_demo_sk\",IntegerType(),True),StructField(\"hd_income_band_sk\",IntegerType(),True),StructField(\"hd_buy_potential\",StringType(),True),StructField(\"hd_dep_count\",IntegerType(),True),StructField(\"hd_vehicle_count\",IntegerType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_household_demographics = spark.read.format(\"csv\").schema(schema_household_demographics).load(\"./work/data/household_demographics*.dat\", delimiter=\"|\")            \n",
    "csv_household_demographics.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"household_demographics\",keyspace=\"tpcds\").mode(\"append\").save()  \n",
    "\n",
    "print(\"schema_income_band\")\n",
    "\n",
    "schema_income_band= StructType([StructField(\"ib_income_band_sk\",IntegerType(),True),StructField(\"ib_lower_bound\",IntegerType(),True),StructField(\"ib_upper_bound\",IntegerType(),True),StructField(\"junk\",StringType(),True),])            \n",
    "csv_income_band = spark.read.format(\"csv\").schema(schema_income_band).load(\"./work/data/income_band*.dat\", delimiter=\"|\")            \n",
    "csv_income_band.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"income_band\",keyspace=\"tpcds\").mode(\"append\").save() \n",
    "\n",
    "\n",
    "## REAL TIME TABLE TO BE USED WITH NOSQLBENCH\n",
    "# schema_web_sales = StructType([StructField(\"ws_sold_date_sk\",IntegerType(),True),StructField(\"ws_sold_time_sk\",IntegerType(),True),StructField(\"ws_ship_date_sk\",IntegerType(),True),StructField(\"ws_item_sk\",IntegerType(),True),StructField(\"ws_bill_customer_sk\",IntegerType(),True),StructField(\"ws_bill_cdemo_sk\",IntegerType(),True),StructField(\"ws_bill_hdemo_sk\",IntegerType(),True),StructField(\"ws_bill_addr_sk\",IntegerType(),True),StructField(\"ws_ship_customer_sk\",IntegerType(),True),StructField(\"ws_ship_cdemo_sk\",IntegerType(),True),StructField(\"ws_ship_hdemo_sk\",IntegerType(),True),StructField(\"ws_ship_addr_sk\",IntegerType(),True),StructField(\"ws_web_page_sk\",IntegerType(),True),StructField(\"ws_web_site_sk\",IntegerType(),True),StructField(\"ws_ship_mode_sk\",IntegerType(),True),StructField(\"ws_warehouse_sk\",IntegerType(),True),StructField(\"ws_promo_sk\",IntegerType(),True),StructField(\"ws_order_number\",IntegerType(),True),StructField(\"ws_quantity\",IntegerType(),True),StructField(\"ws_wholesale_cost\",FloatType(),True),StructField(\"ws_list_price\",FloatType(),True),StructField(\"ws_sales_price\",FloatType(),True),StructField(\"ws_ext_discount_amt\",FloatType(),True),StructField(\"ws_ext_sales_price\",FloatType(),True),StructField(\"ws_ext_wholesale_cost\",FloatType(),True),StructField(\"ws_ext_list_price\",FloatType(),True),StructField(\"ws_ext_tax\",FloatType(),True),StructField(\"ws_coupon_amt\",FloatType(),True),StructField(\"ws_ext_ship_cost\",FloatType(),True),StructField(\"ws_net_paid\",FloatType(),True),StructField(\"ws_net_paid_inc_tax\",FloatType(),True),StructField(\"ws_net_paid_inc_ship\",FloatType(),True),StructField(\"ws_net_paid_inc_ship_tax\",FloatType(),True),StructField(\"ws_net_profit\",FloatType(),True),StructField(\"junk\",StringType(),True),])\n",
    "# csv_web_sales = spark.read.format(\"csv\").schema(schema_web_sales).load(\"/home/ricardo/tpcds-kit/./work/data/out/web_sales*.dat\", delimiter=\"|\")\n",
    "# csv_web_sales.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_sales\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "# print(\"catalog_sales\")\n",
    "# catalog_sales_schema = StructType([ StructField(\"cs_sold_date_sk\",IntegerType(),True), StructField(\"cs_sold_time_sk\",IntegerType(),True), StructField(\"cs_ship_date_sk\",IntegerType(),True), StructField(\"cs_bill_customer_sk\",IntegerType(),True), StructField(\"cs_bill_cdemo_sk\",IntegerType(),True), StructField(\"cs_bill_hdemo_sk\",IntegerType(),True), StructField(\"cs_bill_addr_sk\",IntegerType(),True), StructField(\"cs_ship_customer_sk\",IntegerType(),True), StructField(\"cs_ship_cdemo_sk\",IntegerType(),True), StructField(\"cs_ship_hdemo_sk\",IntegerType(),True), StructField(\"cs_ship_addr_sk\",IntegerType(),True), StructField(\"cs_call_center_sk\",IntegerType(),True), StructField(\"cs_catalog_page_sk\",IntegerType(),True), StructField(\"cs_ship_mode_sk\",IntegerType(),True), StructField(\"cs_warehouse_sk\",IntegerType(),True), StructField(\"cs_item_sk\",IntegerType(),True), StructField(\"cs_promo_sk\",IntegerType(),True), StructField(\"cs_order_number\",IntegerType(),True), StructField(\"cs_quantity\",IntegerType(),True), StructField(\"cs_wholesale_cost\",FloatType(),True), StructField(\"cs_list_price\",FloatType(),True), StructField(\"cs_sales_price\",FloatType(),True), StructField(\"cs_ext_discount_amt\",FloatType(),True), StructField(\"cs_ext_sales_price\",FloatType(),True), StructField(\"cs_ext_wholesale_cost\",FloatType(),True), StructField(\"cs_ext_list_price\",FloatType(),True), StructField(\"cs_ext_tax\",FloatType(),True), StructField(\"cs_coupon_amt\",FloatType(),True), StructField(\"cs_ext_ship_cost\",FloatType(),True), StructField(\"cs_net_paid\",FloatType(),True), StructField(\"cs_net_paid_inc_tax\",FloatType(),True), StructField(\"cs_net_paid_inc_ship\",FloatType(),True), StructField(\"cs_net_paid_inc_ship_tax\",FloatType(),True), StructField(\"cs_net_profit\",FloatType(),True), StructField(\"junk\",StringType(),True), ])\n",
    "# csv_catalog_sales = spark.read.format(\"csv\").schema(catalog_sales_schema).load(\"./work/data/catalog_sales*.dat\", delimiter=\"|\")\n",
    "# csv_catalog_sales.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"catalog_sales\",keyspace=\"tpcds\").mode(\"append\").save()\n",
    "\n",
    "\n",
    "# print(\"schema_store_returns\")\n",
    "# schema_store_returns = StructType([ StructField(\"sr_returned_date_sk\",IntegerType(),True), StructField(\"sr_return_time_sk\",IntegerType(),True), StructField(\"sr_item_sk\",IntegerType(),True), StructField(\"sr_customer_sk\",IntegerType(),True), StructField(\"sr_cdemo_sk\",IntegerType(),True), StructField(\"sr_hdemo_sk\",IntegerType(),True), StructField(\"sr_addr_sk\",IntegerType(),True), StructField(\"sr_store_sk\",IntegerType(),True), StructField(\"sr_reason_sk\",IntegerType(),True), StructField(\"sr_ticket_number\",IntegerType(),True), StructField(\"sr_return_quantity\",IntegerType(),True), StructField(\"sr_return_amt\",FloatType(),True), StructField(\"sr_return_tax\",FloatType(),True), StructField(\"sr_return_amt_inc_tax\",FloatType(),True), StructField(\"sr_fee\",FloatType(),True), StructField(\"sr_return_ship_cost\",FloatType(),True), StructField(\"sr_refunded_cash\",FloatType(),True), StructField(\"sr_reversed_charge\",FloatType(),True), StructField(\"sr_store_credit\",FloatType(),True), StructField(\"sr_net_loss\",FloatType(),True), StructField(\"junk\",StringType(),True)])\n",
    "# csv_store_returns = spark.read.format(\"csv\").schema(schema_store_returns).load(\"./work/data/store_returns*.dat\", delimiter=\"|\")\n",
    "# csv_store_returns.write .format(\"org.apache.spark.sql.cassandra\") .options(table=\"store_returns\",keyspace=\"tpcds\") .mode(\"append\").save()   \n",
    "\n",
    "\n",
    "# print(\"schema_web_returns\")\n",
    "# schema_web_returns = StructType([StructField(\"wr_returned_date_sk\",IntegerType(),True),StructField(\"wr_returned_time_sk\",IntegerType(),True),StructField(\"wr_item_sk\",IntegerType(),True),StructField(\"wr_refunded_customer_sk\",IntegerType(),True),StructField(\"wr_refunded_cdemo_sk\",IntegerType(),True),StructField(\"wr_refunded_hdemo_sk\",IntegerType(),True),StructField(\"wr_refunded_addr_sk\",IntegerType(),True),StructField(\"wr_returning_customer_sk\",IntegerType(),True),StructField(\"wr_returning_cdemo_sk\",IntegerType(),True),StructField(\"wr_returning_hdemo_sk\",IntegerType(),True),StructField(\"wr_returning_addr_sk\",IntegerType(),True),StructField(\"wr_web_page_sk\",IntegerType(),True),StructField(\"wr_reason_sk\",IntegerType(),True),StructField(\"wr_order_number\",IntegerType(),True),StructField(\"wr_return_quantity\",IntegerType(),True),StructField(\"wr_return_amt\",FloatType(),True),StructField(\"wr_return_tax\",FloatType(),True),StructField(\"wr_return_amt_inc_tax\",FloatType(),True),StructField(\"wr_fee\",FloatType(),True),StructField(\"wr_return_ship_cost\",FloatType(),True),StructField(\"wr_refunded_cash\",FloatType(),True),StructField(\"wr_reversed_charge\",FloatType(),True),StructField(\"wr_account_credit\",FloatType(),True),StructField(\"wr_net_loss\",FloatType(),True),StructField(\"junk\",StringType(),True),])\n",
    "# csv_web_returns = spark.read.format(\"csv\").schema(schema_web_returns).load(\"/home/ricardo/tpcds-kit/./work/data/out/web_returns*.dat\", delimiter=\"|\")\n",
    "# csv_web_returns.write .format(\"org.apache.spark.sql.cassandra\") .options(table=\"web_returns\",keyspace=\"tpcds\") .mode(\"append\").save()\n",
    "\n",
    "# print(\"store_sales\")\n",
    "# schema_store_sales = StructType([ StructField(\"ss_sold_date_sk\",IntegerType(),True), StructField(\"ss_sold_time_sk\",IntegerType(),True), StructField(\"ss_item_sk\",IntegerType(),True), StructField(\"ss_customer_sk\",IntegerType(),True), StructField(\"ss_cdemo_sk\",IntegerType(),True), StructField(\"ss_hdemo_sk\",IntegerType(),True), StructField(\"ss_addr_sk\",IntegerType(),True), StructField(\"ss_store_sk\",IntegerType(),True), StructField(\"ss_promo_sk\",IntegerType(),True), StructField(\"ss_ticket_number\",IntegerType(),True), StructField(\"ss_quantity\",IntegerType(),True), StructField(\"ss_wholesale_cost\",FloatType(),True), StructField(\"ss_list_price\",FloatType(),True), StructField(\"ss_sales_price\",FloatType(),True), StructField(\"ss_ext_discount_amt\",FloatType(),True), StructField(\"ss_ext_sales_price\",FloatType(),True), StructField(\"ss_ext_wholesale_cost\",FloatType(),True), StructField(\"ss_ext_list_price\",FloatType(),True), StructField(\"ss_ext_tax\",FloatType(),True), StructField(\"ss_coupon_amt\",FloatType(),True), StructField(\"ss_net_paid\",FloatType(),True), StructField(\"ss_net_paid_inc_tax\",FloatType(),True), StructField(\"ss_net_profit\",FloatType(),True), StructField(\"junk\",StringType(),True),  ])\n",
    "# csv_store_sales = spark.read.format(\"csv\").schema(schema_store_sales).load(\"./work/data/store_sales*.dat\", delimiter=\"|\")\n",
    "# csv_store_sales.write .format(\"org.apache.spark.sql.cassandra\") .options(table=\"store_sales\",keyspace=\"tpcds\") .mode(\"append\").save() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Real-time Workload with NoSQLBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On your Terminal start the workload:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./nb run driver=cql workload=/home/jovyan/work/workload-nosqlbench.yaml tags=phase:schema threads=auto cycles=11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Analytical SQL Queries with SparkSQL and Scylla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "IPS = ['172.19.0.2']\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"IoT-Scylla\")\\\n",
    "    .config(\"setMaster\",\"local[*]\")\\\n",
    "    .config(\"spark.jars\", \"target/scala-2.12/spark3-scylla4-example-assembly-0.1.jar\")\\\n",
    "    .config(\"spark.cassandra.connection.host\", ','.join(IPS))\\\n",
    "    .config('spark.cassandra.concurrent.reads','20480')\\\n",
    "    .config('spark.cassandra.input.consistency.level','LOCAL_ONE')\\\n",
    "    .config('spark.cassandra.input.fetch.sizeInRows','2000')\\\n",
    "    .config('spark.cassandra.input.split.sizeInMB','512')\\\n",
    "    .config(\"spark.driver.memory\", \"5g\")\\\n",
    "    .config(\"spark.executor.memory\", \"5g\")\\\n",
    "    .config(\"spark.driver.cores\",5)\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "sc._conf.get('spark.executor.memory')\n",
    "sc._conf.get('spark.driver.memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Registering Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_center = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"call_center\", keyspace=\"tpcds\").load()\n",
    "call_center.registerTempTable(\"call_center\")\n",
    "catalog_page = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"catalog_page\", keyspace=\"tpcds\").load()\n",
    "catalog_page.registerTempTable(\"catalog_page\")\n",
    "catalog_returns = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"catalog_returns\", keyspace=\"tpcds\").load()\n",
    "catalog_returns.registerTempTable(\"catalog_returns\")\n",
    "catalog_sales = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"catalog_sales\", keyspace=\"tpcds\").load()\n",
    "catalog_sales.registerTempTable(\"catalog_sales\")\n",
    "customer = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer\", keyspace=\"tpcds\").load()\n",
    "customer.registerTempTable(\"customer\")\n",
    "customer_address = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_address\", keyspace=\"tpcds\").load()\n",
    "customer_address.registerTempTable(\"customer_address\")\n",
    "customer_demographics = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_demographics\", keyspace=\"tpcds\").load()\n",
    "customer_demographics.registerTempTable(\"customer_demographics\")\n",
    "date_dim = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"date_dim\", keyspace=\"tpcds\").load()\n",
    "date_dim.registerTempTable(\"date_dim\")\n",
    "household_demographics = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"household_demographics\", keyspace=\"tpcds\").load()\n",
    "household_demographics.registerTempTable(\"household_demographics\")\n",
    "income_band = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"income_band\", keyspace=\"tpcds\").load()\n",
    "income_band.registerTempTable(\"income_band\")\n",
    "inventory = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"inventory\", keyspace=\"tpcds\").load()\n",
    "inventory.registerTempTable(\"inventory\")\n",
    "item = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"item\", keyspace=\"tpcds\").load()\n",
    "item.registerTempTable(\"item\")\n",
    "promotion = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"promotion\", keyspace=\"tpcds\").load()\n",
    "promotion.registerTempTable(\"promotion\")\n",
    "reason = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"reason\", keyspace=\"tpcds\").load()\n",
    "reason.registerTempTable(\"reason\")\n",
    "ship_mode = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"ship_mode\", keyspace=\"tpcds\").load()\n",
    "ship_mode.registerTempTable(\"ship_mode\")\n",
    "store = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"store\", keyspace=\"tpcds\").load()\n",
    "store.registerTempTable(\"store\")\n",
    "store_returns = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"store_returns\", keyspace=\"tpcds\").load()\n",
    "store_returns.registerTempTable(\"store_returns\")\n",
    "store_sales = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"store_sales\", keyspace=\"tpcds\").load()\n",
    "store_sales.registerTempTable(\"store_sales\")\n",
    "time_dim = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"time_dim\", keyspace=\"tpcds\").load()\n",
    "time_dim.registerTempTable(\"time_dim\")\n",
    "warehouse = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"warehouse\", keyspace=\"tpcds\").load()\n",
    "warehouse.registerTempTable(\"warehouse\")\n",
    "web_page = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_page\", keyspace=\"tpcds\").load()\n",
    "web_page.registerTempTable(\"web_page\")\n",
    "web_returns = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_returns\", keyspace=\"tpcds\").load()\n",
    "web_returns.registerTempTable(\"web_returns\")\n",
    "web_sales = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_sales\", keyspace=\"tpcds\").load()\n",
    "web_sales.registerTempTable(\"web_sales\")\n",
    "web_site = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"web_site\", keyspace=\"tpcds\").load()\n",
    "web_site.registerTempTable(\"web_site\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = '''select d_date from web_sales join date_dim on ws_sold_date_sk = d_date_sk\n",
    "                              where d_date between \\'2021-06-20' and \\'2021-06-21\\''''\n",
    "sqlContext.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query1= '''select \n",
    "    i_item_desc,\n",
    "  i_category,\n",
    "  i_class,\n",
    "  i_current_price,\n",
    "  i_item_id,\n",
    "  sum(ws_ext_sales_price) as itemrevenue\n",
    "from\n",
    "  web_sales\n",
    "  join item on (web_sales.ws_item_sk = item.i_item_sk)\n",
    "where\n",
    "  i_category in('Jewelry', 'Sports', 'Books')\n",
    "  and ws_sold_date_sk = 2459247\n",
    "group by\n",
    "  i_item_id,\n",
    "  i_item_desc,\n",
    "  i_category,\n",
    "  i_class,\n",
    "  i_current_price\n",
    "order by\n",
    "  i_category,\n",
    "  i_class,\n",
    "  i_item_id,\n",
    "  i_item_desc\n",
    "  -- revenueratio\n",
    "limit 1000;'''\n",
    "\n",
    "sqlContext.sql(query1).show(100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
